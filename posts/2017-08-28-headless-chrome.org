---
title: 使用headless chromium来抓取网页
tags: python,web
---
#+OPTIONS: ^:nil

在使用[[https://scrapy.org/][scrapy]]编写爬虫的过程中遇到一些动态加载的内容不好抓取，所以研究了一下[[https://chromium.googlesource.com/chromium/src/+/lkgr/headless/README.md][headless chromium]]，如果不用这种方式的话就得手动
破解ajax请求的接口，了解不同网站做的校验规则，使用浏览器解析dom然后分析页面结构是最简单快速的方法了。

* headless chromium基本用法
根据[[https://developers.google.com/web/updates/2017/04/headless-chrome][官方教程]]说明，在chromium59版本之后的浏览器自带了无头模式，只要通过命令行执行

=chromium --headless --disable-gpu=

就能打开一个无头的浏览器了，如果我想打开某个网页，并且分析完全加载完成的dom结构，那么只要执行

 =chromium --headless --disable-gpu --dump-dom http://www.dianping.com/shop/14198848 > /tmp/14198848.html=

想要做后续的分析直接分析这个html文件就行了

* 通过selenium来驱动headless chromium
python有个叫selenium的包叫作，安装好之后就能操作浏览器了
** 安装依赖
=sudo pip install selenium=
** 示例代码
#+BEGIN_SRC python
  from selenium import webdriver
  from selenium.webdriver.chrome.options import Options

  chrome_options = Options()
  chrome_options.add_argument("--headless")

  driver = webdriver.Chrome(chrome_options=chrome_options)
  driver.get('http://www.dianping.com/shop/59473758')
  html = driver.execute_script("return document.documentElement.outerHTML")

  id = '59474758'
  with open(id + '.html', 'wb') as f:
      f.write(str.encode(html))

  driver.save_screenshot(id + '.png')
  driver.quit()
#+END_SRC
这段脚本的作用很明显，首先是创建一个headless chromium的浏览器对象，然后用它打开大众点评的一个网页，然后导出dom，写入到一个html文件
中，最后把当前网页截图保存为一个png文件，并且关闭浏览器

* scrapy操作字符串
通过headless chromium导出的是纯字符串，如果要让它支持xpath解析，需要处理一下，比如：
1. [[https://stackoverflow.com/questions/27323740/scrapy-convert-html-string-to-htmlresponse-object][转成HtmlResponse对象]]
#+BEGIN_SRC python
  from scrapy.http import HtmlResponse

  response = HtmlResponse(url="my HTML string", body='<div id="test">Test text</div>')
  response.xpath('//div[@id="test"]/text()').extract()[0].strip()
#+END_SRC

2. [[https://stackoverflow.com/questions/8711030/fetch-partial-string-matched-html-tag-using-xpath][转成lxml对象]]
#+BEGIN_SRC python
  import lxml

  html = '<div id="test">Test text</div>'
  doc = lxml.html.fromstring(html)
#+END_SRC

有了headless chromium之后写爬虫就方便多了，甚至现代的各种SPA应用也不用担心，happy hacking!
