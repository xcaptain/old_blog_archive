---
layout: post
title: 又是一个采集
date: 2014-12-07 22:25:45 +0000
comments: true
tags: python
---

前几天看到有个同事在采集[这个](https://yande.re)站，要把上面的图片都搞下来。看他很得意的样子，似乎采集是个什么很复杂东西，其实简单得很，没什么技术含量在里面。他用的是一个叫做scrapy的采集框架，这个框架我没用过，但是自己写采集也简单。

之前采集过多玩和178的一些文章，需求还比较复杂，需要把ajax的接口也给搞定掉，还得把所有图片都本地化，那时候用的是urllib2+beautifulsoup这个东西采集的。

今天闲着没事，打算再来搞个采集玩玩，[yande](https://yande.re)上面的图片质量还是比较高的。还是使用老一套，不过这回我不打算使用urllib2了，这东西太弱了，编码问题总是会出现，搞得我莫名其妙的，实在是打算放弃urllib2了，上网搜一下，找到了一个叫做`requests`的网络库，这个东西目前开发还是蛮活跃的，而且口气比较大，直接在官网说urllib2的api很混乱，那就是它了。页面分析的库还是用回beautifulsoup，这么久了仍然是很不喜欢写正则，所以拿这个库来解析html还是比较顺手。至于信息入库，我用的是`MySQLdb`这个库来驱动mysql，似乎这个库也比较弱，反正我是遇到很多很简单的sql语句都报错的情况了，在网上搜了一下，发现别的第三方的mysql库也很多，但是还没有下定决心换。

代码的思路很简单，和以前写的采集脚本差不多，都是先采集列表页，获得文章页的url，然后在文章页中采集想要的信息。信息采集完了就是入库，到这里采集过程就结束了。

代码很简单，已经提交到[github](https://github.com/xcaptain/yande)了，和之前写的脚本比起来，多增加了一些东西：
1. 模糊匹配，比如说`soup.find(href=re.compile('user\/show'))`这样的东西。
2. 断点续踩，程序在跑的时候如果按下`Ctrl-C`来终止它的话，会把当前采集到的页码写入到一个叫做`config.py`的文件。
3. 异常处理，以前写的几个采集使用了很多if语句，很多嵌套的判断，就是为了人为规避异常，尽量让程序跑的时候都不遇到异常，这种写法真是太折磨人了，看到三四层嵌套判断我的头都要大了。这回尝试了一下直接在`try`里面执行代码段，如果遇到可能会有的异常就直接`except`上处理了它，真是省了很多心呀。
