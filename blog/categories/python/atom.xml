<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Joey's Blog]]></title>
  <link href="http://xcaptain.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://xcaptain.github.io/"/>
  <updated>2015-03-15T01:26:39+08:00</updated>
  <id>http://xcaptain.github.io/</id>
  <author>
    <name><![CDATA[joey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python2编码问题]]></title>
    <link href="http://xcaptain.github.io/blog/2015/01/13/python-encoding/"/>
    <updated>2015-01-13T08:06:21+08:00</updated>
    <id>http://xcaptain.github.io/blog/2015/01/13/python-encoding</id>
    <content type="html"><![CDATA[<p>这篇文章是要记录今天写一个采集是遇到的编码问题的，但是因为在写博客时遇到了一些问题，所以也就顺便记录一下。</p>

<p>在执行rake new_post的时候，突然提示</p>

<pre>
rake aborted!
LoadError: cannot load such file -- bundler/setup
/home/joey/octopress/Rakefile:2:in `<top (required)>'
(See full trace by running task with --trace)
</pre>


<p>突然感觉很奇怪，前几天更新博客的时候都没有遇到这种情况，后来执行<code>rake --trace</code>的时候发现</p>

<pre>
/usr/lib/ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:in `require'
/usr/lib/ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:in `require'
/home/joey/octopress/Rakefile:2:in `<top (required)>'
/usr/lib/ruby/2.2.0/rake/rake_module.rb:28:in `load'
/usr/lib/ruby/2.2.0/rake/rake_module.rb:28:in `load_rakefile'
/usr/lib/ruby/2.2.0/rake/application.rb:689:in `raw_load_rakefile'
/usr/lib/ruby/2.2.0/rake/application.rb:94:in `block in load_rakefile'
/usr/lib/ruby/2.2.0/rake/application.rb:176:in `standard_exception_handling'
/usr/lib/ruby/2.2.0/rake/application.rb:93:in `load_rakefile'
/usr/lib/ruby/2.2.0/rake/application.rb:77:in `block in run'
/usr/lib/ruby/2.2.0/rake/application.rb:176:in `standard_exception_handling'
/usr/lib/ruby/2.2.0/rake/application.rb:75:in `run'
/usr/bin/rake:33:in `<main>'
</pre>


<p>原来是系统ruby的版本更新到2.2了，以前是2.1的，octopress用的是2.1的gem，也许是和2.2的ruby不兼容吧，所以要重装2.2的gem套系，执行<code>proxychains gem install bundle</code>，有经验了必须要通过代理才能访问rubygem网站，下载完之后需要把<code>/home/joey/.gem/ruby/2.2.0/bin</code> 添加到环境变量中，因为我用的是fish，所以编辑<code>~/.config/fish/config.fish</code>就行了。然后用bundle安装完必要的依赖<code>proxychains bundle install</code>，下载一大堆gem，最后发现系统用的rake已经是10.4.2版的了，而配置文件中需要的还是10.4.0,手动编辑一下<code>Gemfile.lock</code>，把版本号改过来就行了，后来又重装了一下<code>safe_yaml</code>和<code>liquid</code>，把对应的版本号也都改过来了，一个小版本升级搞得这么麻烦，真是郁闷。</p>

<p>接下来就是说说今天写代码遇到的问题了。</p>

<p>需求很简单（话说写采集需求都很明确），这回需要的是采集明星的信息，第一个方案用的是采集baidu整理的明星信息，过程很简单，写了一个简单的脚本采了900条数据，但是后来发现图片做了防盗链，采集过来的图片不是图片的绝对路径，而是通过一台服务器生成的图片，也许是做了cookie的限制或者是做了ip的限制，导致每次刷新页面的时候都会返回一个403错误，链接在<a href="http://www.baidu.com/s?wd=%E6%98%8E%E6%98%9F%E5%A4%A7%E5%85%A8&amp;rsv_spt=1&amp;issp=1&amp;f=8&amp;rsv_bp=0&amp;rsv_idx=2&amp;ie=utf-8&amp;tn=baiduhome_pg&amp;rsv_enter=1&amp;rsv_sug3=4&amp;rsv_sug4=83&amp;rsv_sug1=3&amp;rsv_pq=d5fdf7cd00002ac7&amp;rsv_t=975c0jUWTeYgIUYA%2FfdqSJ75f%2BipUP9QR9v8Qgqb2jzy3rnHgTU3k4rBD%2B5moP73i00p&amp;rsv_sug2=0&amp;inputT=5564">这里</a>，以后有时间也许会去看看这个防盗链的实现，但是现在我可懒得花时间去研究怎么破解这个限制，那就换别的站采集吧。</p>

<p>和产品沟通了一下，他也觉得如果死扣baidu是不明智的选择，后来换成360整理的资源了，链接在<a href="http://www.haosou.com/s?ie=utf-8&amp;shb=1&amp;src=360sou_newhome&amp;q=%E6%98%8E%E6%98%9F%E5%A4%A7%E5%85%A8">这里</a>，数据也比较明确，分成3栏：领域，地域，性别，这个分类还是比较好的，以后我们拿来查询也方便。通过chrome开发工具，找到了js的接口，简单分析了一下就知道要获取那块数据，接下来就是敲代码了，最后的代码在<a href="https://gist.github.com/xcaptain/cbf9980f1b30b2467d8a">这里</a>，本来是打算试试octopress对于gist的支持的，但是因为gist在国内被墙了，会影响整篇文章的阅读，所以就换个链接在这里，有兴趣的朋友就去看看。</p>

<p>在写代码过程中遇到一个编码问题，困扰我很久，不过最终还是解决了。</p>

<p>在使用python2的时候，要自己手动设置编码，python默认的字符串编码是ascii，这就导致如果在python2程序中出现了中文都会提示一个语法错误，但是如果在python2文件中强制加上一句<code># -*- coding: utf-8 -*-</code> 这样python在执行程序的时候就知道使用utf-8来编码里面的字符串了。也许是我本地locale的设置，在打开python解释器的时候总是会自动帮我使用utf-8编码。
首先说说ascii编码，ascii只能编码代码点从0到127的字符，也就是英文字符，如果遇到代码点很高的字符，比如说中文，就没法争取的编码了，就会报错。下面来举几个例子：</p>

<pre><code class="python2">s1 = '你好，world' #这里如果指定了文件的编码为utf-8，会自动把这个字符串编码成utf-8
s1 =&gt; '\xe4\xbd\xa0\xe5\xa5\xbd\xef\xbc\x8cworld'
s2 = u'你好，world' #这里前面加了一个u来表明这个字符串是一个unicode字符串
s2 =&gt; u'\u4f60\u597d\uff0cworld'
u2 = s2.encode('utf-8')
u2 =&gt; '\xe4\xbd\xa0\xe5\xa5\xbd\xef\xbc\x8cworld'
</code></pre>

<p>字符串在内存中应该是用类似utf-8的形式存储的，这样比较节省内存空间，而且也不会出现太多的0导致字符串在大尾和小尾机器上的不兼容。所以对于一个python字符串的旅程可以大致归结为：</p>

<ol>
<li>在编辑器里写下<code>s1 = '你好，world'</code>然后保存为一个python文件的时候，编辑器会自动选择某种编码来保存这个文件，一般来说都是用的utf-8。</li>
<li>python把这个程序加载进解释器的时候，会根据文件头来判断使用什么编码来解码这个文件，也就是<code># -*-coding: utf-8 -*-</code>这行，如果没有这行就会用默认的<code>ascii</code>来解码。</li>
<li>这时候程序里的字符串都是utf8的。
写到这里突然发现昨晚的问题实在不算问题，看来之前是没有静下心来研究，写博客还是有好处的。</li>
</ol>


<p>接下来再看看python3,python3的默认编码是unicode，而实际存在内存中的就是字节码，bytecode。也就是说一个字符串只有2种状态，unicode和byte，这样就节省很多事情了，没有各种编码解码的麻烦。
<code>python3
s = '你好，world' #这是一个str类型的字符串
b1 = s.encode('utf-8') #这是把s编码为utf-8后的字节码
b2 = s.encode('gbk') #把s编码为gbk后的字节码
</code>
相对于2来说，3最大的进步就是不需要手动encode，decode，对于处理未知编码的文件最方便了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[采集一个论坛的帖子来进行数据分析]]></title>
    <link href="http://xcaptain.github.io/blog/2014/12/31/web-crawl-again/"/>
    <updated>2014-12-31T07:49:40+08:00</updated>
    <id>http://xcaptain.github.io/blog/2014/12/31/web-crawl-again</id>
    <content type="html"><![CDATA[<p>今天接到一个小小的需求，又是和采集相关的，我都要烦死了，采集这东西写得都要腻了。</p>

<p>不过和以前的那些采集有点区别，不再是采集文章了，而是开始采集论坛的帖子。说实话我觉得采集论坛比较简单，因为论坛都是php的页面，页面组织布局一般不会怎么变，而新闻站基本都是cms系统生成的html静态页，静态的页面就有太多可能性了，而且不排除有的编辑太敬业把每个页面都做得不一样。</p>

<p>采集源站是<a href="http://qq.100bt.com">http://qq.100bt.com</a>，和我最近做的<a href="http://www.zeze.com">zeze</a>是同一类型的东西，而且突然发现zeze竟然借鉴了很多这个网站的元素。废话不多说，这次采集主要是为了了解哪些资讯比较受欢迎，然后我们就可以有针对性的来推广我们的内容，看起来像是在收集竞争对手的信息而实现知己知彼百战不殆，这样的活我喜欢。这个叫作百田圈圈的网站似乎运营得不错，里面积累了差不多百万的帖子，用户似乎也很活跃。</p>

<p>其实要获得的信息不多，在我设计的数据库结构中包含以下字段：</p>

<pre>
tid: 帖子的id，用来作为表的主键
url: 帖子的url，便于访问
subject: 帖子标题
author_name: 楼主名，有这个东西可以用来分析哪些用户比较活跃
replies: 帖子的回复数，这个字段是用来衡量一个帖子的受欢迎程度的最重要指标
view: 帖子被查看的次数
pubdate: 发帖时间
maxpage: 帖子页数，因为有的帖子是回复楼层的，所以如果一个帖子分了很多页说明这个帖子内容丰富
category: 帖子所属分类，用来来分析哪些大类的话题比较火
</pre>


<p>没有数据库直接操作真是麻烦，上面这些数据都得分析网页去获取，而且不是在同一个页面，需求挺麻烦的但是不难。</p>

<p>代码传到<a href="https://github.com/xcaptain/BeautifulSoup_Instance/tree/master/btqq">github</a>上面了，以前写的那些采集都是用脚本的形式来实现的，这回尝试了一下自己构造一个采集类，这时候才发现对于python的面向对象特性用的太少了，在设计类的结构，接口上面都很不熟练，现在的代码设计在python大牛严重肯定是很小儿科的，以后要多学习OOP的编程思想，不能总是一个函数一个函数这样顺序的执行下来。</p>

<p>写这么一个简单的任务竟然还花了我一天的时间，看来编码能力还是有待加强，经过总结之后发现问题主要存在三方面：</p>

<ol>
<li>对mysql的查询语句不熟悉，因为我把tid设置为了主键，所以不能重复插入，我想要实现<code>insert into on duplicate key continue</code>的功能，找了半天只看到了<code>insert into on duplicate key update</code>的语法，但是后者不是我想要的，因为使用的是python的<code>MySQLdb</code>来驱动mysql的，这东西对于原生的sql语句支持不是很好，不过幸好最后我找到了<code>insert ignore into</code>这种语句，一下解决了问题，虽然<code>MySQLdb</code>在<code>execute</code>的时候会有警告，但是毕竟实现了功能。</li>
<li><code>MySQLdb</code>这个库的使用不熟悉，在用它来执行sql语句的时候出现了很多问题，看来有时间得专门总结一下python驱动mysql的方法。</li>
</ol>


<p>今晚下班回的路上突然发现代码少写了很多异常检测，比如说没有判断网络链接失败，页面匹配为空等，真希望程序能够按照我设想的执行，等明天早上到了公司6张表都给我装得满满的，明天到了公司 再去修改代码了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[又是一个采集]]></title>
    <link href="http://xcaptain.github.io/blog/2014/12/08/learn-beautifulsoup/"/>
    <updated>2014-12-08T06:25:45+08:00</updated>
    <id>http://xcaptain.github.io/blog/2014/12/08/learn-beautifulsoup</id>
    <content type="html"><![CDATA[<p>前几天看到有个同事在采集<a href="https://yande.re">这个</a>站，要把上面的图片都搞下来。看他很得意的样子，似乎采集是个什么很复杂东西，其实简单得很，没什么技术含量在里面。他用的是一个叫做scrapy的采集框架，这个框架我没用过，但是自己写采集也简单。</p>

<p>之前采集过多玩和178的一些文章，需求还比较复杂，需要把ajax的接口也给搞定掉，还得把所有图片都本地化，那时候用的是urllib2+beautifulsoup这个东西采集的。</p>

<p>今天闲着没事，打算再来搞个采集玩玩，<a href="https://yande.re">yande</a>上面的图片质量还是比较高的。还是使用老一套，不过这回我不打算使用urllib2了，这东西太弱了，编码问题总是会出现，搞得我莫名其妙的，实在是打算放弃urllib2了，上网搜一下，找到了一个叫做<code>requests</code>的网络库，这个东西目前开发还是蛮活跃的，而且口气比较大，直接在官网说urllib2的api很混乱，那就是它了。页面分析的库还是用回beautifulsoup，这么久了仍然是很不喜欢写正则，所以拿这个库来解析html还是比较顺手。至于信息入库，我用的是<code>MySQLdb</code>这个库来驱动mysql，似乎这个库也比较弱，反正我是遇到很多很简单的sql语句都报错的情况了，在网上搜了一下，发现别的第三方的mysql库也很多，但是还没有下定决心换。</p>

<p>代码的思路很简单，和以前写的采集脚本差不多，都是先采集列表页，获得文章页的url，然后在文章页中采集想要的信息。信息采集完了就是入库，到这里采集过程就结束了。</p>

<p>代码很简单，已经提交到<a href="https://github.com/xcaptain/yande">github</a>了，和之前写的脚本比起来，多增加了一些东西：
1. 模糊匹配，比如说<code>soup.find(href=re.compile('user\/show'))</code>这样的东西。
2. 断点续踩，程序在跑的时候如果按下<code>Ctrl-C</code>来终止它的话，会把当前采集到的页码写入到一个叫做<code>config.py</code>的文件。
3. 异常处理，以前写的几个采集使用了很多if语句，很多嵌套的判断，就是为了人为规避异常，尽量让程序跑的时候都不遇到异常，这种写法真是太折磨人了，看到三四层嵌套判断我的头都要大了。这回尝试了一下直接在<code>try</code>里面执行代码段，如果遇到可能会有的异常就直接<code>except</code>上处理了它，真是省了很多心呀。</p>
]]></content>
  </entry>
  
</feed>
