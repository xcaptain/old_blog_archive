<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Joey's Blog]]></title>
  <link href="http://xcaptain.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://xcaptain.github.io/"/>
  <updated>2015-01-05T23:55:50+00:00</updated>
  <id>http://xcaptain.github.io/</id>
  <author>
    <name><![CDATA[joey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[采集一个论坛的帖子来进行数据分析]]></title>
    <link href="http://xcaptain.github.io/blog/2014/12/30/web-crawl-again/"/>
    <updated>2014-12-30T23:49:40+00:00</updated>
    <id>http://xcaptain.github.io/blog/2014/12/30/web-crawl-again</id>
    <content type="html"><![CDATA[<p>今天接到一个小小的需求，又是和采集相关的，我都要烦死了，采集这东西写得都要腻了。</p>

<p>不过和以前的那些采集有点区别，不再是采集文章了，而是开始采集论坛的帖子。说实话我觉得采集论坛比较简单，因为论坛都是php的页面，页面组织布局一般不会怎么变，而新闻站基本都是cms系统生成的html静态页，静态的页面就有太多可能性了，而且不排除有的编辑太敬业把每个页面都做得不一样。</p>

<p>采集源站是<a href="http://qq.100bt.com">http://qq.100bt.com</a>，和我最近做的<a href="http://www.zeze.com">zeze</a>是同一类型的东西，而且突然发现zeze竟然借鉴了很多这个网站的元素。废话不多说，这次采集主要是为了了解哪些资讯比较受欢迎，然后我们就可以有针对性的来推广我们的内容，看起来像是在收集竞争对手的信息而实现知己知彼百战不殆，这样的活我喜欢。这个叫作百田圈圈的网站似乎运营得不错，里面积累了差不多百万的帖子，用户似乎也很活跃。</p>

<p>其实要获得的信息不多，在我设计的数据库结构中包含以下字段：</p>

<pre>
tid: 帖子的id，用来作为表的主键
url: 帖子的url，便于访问
subject: 帖子标题
author_name: 楼主名，有这个东西可以用来分析哪些用户比较活跃
replies: 帖子的回复数，这个字段是用来衡量一个帖子的受欢迎程度的最重要指标
view: 帖子被查看的次数
pubdate: 发帖时间
maxpage: 帖子页数，因为有的帖子是回复楼层的，所以如果一个帖子分了很多页说明这个帖子内容丰富
category: 帖子所属分类，用来来分析哪些大类的话题比较火
</pre>


<p>没有数据库直接操作真是麻烦，上面这些数据都得分析网页去获取，而且不是在同一个页面，需求挺麻烦的但是不难。</p>

<p>代码传到<a href="https://github.com/xcaptain/BeautifulSoup_Instance/tree/master/btqq">github</a>上面了，以前写的那些采集都是用脚本的形式来实现的，这回尝试了一下自己构造一个采集类，这时候才发现对于python的面向对象特性用的太少了，在设计类的结构，接口上面都很不熟练，现在的代码设计在python大牛严重肯定是很小儿科的，以后要多学习OOP的编程思想，不能总是一个函数一个函数这样顺序的执行下来。</p>

<p>写这么一个简单的任务竟然还花了我一天的时间，看来编码能力还是有待加强，经过总结之后发现问题主要存在三方面：</p>

<ol>
<li>对mysql的查询语句不熟悉，因为我把tid设置为了主键，所以不能重复插入，我想要实现<code>insert into on duplicate key continue</code>的功能，找了半天只看到了<code>insert into on duplicate key update</code>的语法，但是后者不是我想要的，因为使用的是python的<code>MySQLdb</code>来驱动mysql的，这东西对于原生的sql语句支持不是很好，不过幸好最后我找到了<code>insert ignore into</code>这种语句，一下解决了问题，虽然<code>MySQLdb</code>在<code>execute</code>的时候会有警告，但是毕竟实现了功能。</li>
<li><code>MySQLdb</code>这个库的使用不熟悉，在用它来执行sql语句的时候出现了很多问题，看来有时间得专门总结一下python驱动mysql的方法。</li>
</ol>


<p>今晚下班回的路上突然发现代码少写了很多异常检测，比如说没有判断网络链接失败，页面匹配为空等，真希望程序能够按照我设想的执行，等明天早上到了公司6张表都给我装得满满的，明天到了公司 再去修改代码了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[又是一个采集]]></title>
    <link href="http://xcaptain.github.io/blog/2014/12/07/learn-beautifulsoup/"/>
    <updated>2014-12-07T22:25:45+00:00</updated>
    <id>http://xcaptain.github.io/blog/2014/12/07/learn-beautifulsoup</id>
    <content type="html"><![CDATA[<p>前几天看到有个同事在采集<a href="https://yande.re">这个</a>站，要把上面的图片都搞下来。看他很得意的样子，似乎采集是个什么很复杂东西，其实简单得很，没什么技术含量在里面。他用的是一个叫做scrapy的采集框架，这个框架我没用过，但是自己写采集也简单。</p>

<p>之前采集过多玩和178的一些文章，需求还比较复杂，需要把ajax的接口也给搞定掉，还得把所有图片都本地化，那时候用的是urllib2+beautifulsoup这个东西采集的。</p>

<p>今天闲着没事，打算再来搞个采集玩玩，<a href="https://yande.re">yande</a>上面的图片质量还是比较高的。还是使用老一套，不过这回我不打算使用urllib2了，这东西太弱了，编码问题总是会出现，搞得我莫名其妙的，实在是打算放弃urllib2了，上网搜一下，找到了一个叫做<code>requests</code>的网络库，这个东西目前开发还是蛮活跃的，而且口气比较大，直接在官网说urllib2的api很混乱，那就是它了。页面分析的库还是用回beautifulsoup，这么久了仍然是很不喜欢写正则，所以拿这个库来解析html还是比较顺手。至于信息入库，我用的是<code>MySQLdb</code>这个库来驱动mysql，似乎这个库也比较弱，反正我是遇到很多很简单的sql语句都报错的情况了，在网上搜了一下，发现别的第三方的mysql库也很多，但是还没有下定决心换。</p>

<p>代码的思路很简单，和以前写的采集脚本差不多，都是先采集列表页，获得文章页的url，然后在文章页中采集想要的信息。信息采集完了就是入库，到这里采集过程就结束了。</p>

<p>代码很简单，已经提交到<a href="https://github.com/xcaptain/yande">github</a>了，和之前写的脚本比起来，多增加了一些东西：
1. 模糊匹配，比如说<code>soup.find(href=re.compile('user\/show'))</code>这样的东西。
2. 断点续踩，程序在跑的时候如果按下<code>Ctrl-C</code>来终止它的话，会把当前采集到的页码写入到一个叫做<code>config.py</code>的文件。
3. 异常处理，以前写的几个采集使用了很多if语句，很多嵌套的判断，就是为了人为规避异常，尽量让程序跑的时候都不遇到异常，这种写法真是太折磨人了，看到三四层嵌套判断我的头都要大了。这回尝试了一下直接在<code>try</code>里面执行代码段，如果遇到可能会有的异常就直接<code>except</code>上处理了它，真是省了很多心呀。</p>
]]></content>
  </entry>
  
</feed>
